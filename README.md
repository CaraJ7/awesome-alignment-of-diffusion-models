# Awesome Alignment of Diffusion Models

The collection of awesome papers on alignment of diffusion model.

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/zeke-xie/awesome-alignment-of-diffusion-models)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Made With Love](https://img.shields.io/badge/Made%20With-Love-red.svg)](https://github.com/chetanraj/awesome-github-badges)

## Human Alignment of Diffusion Models

* Human Preference Score: Better Aligning Text-to-Image Models with Human Preference
* ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation
* Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis
* Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation
* DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models
* Finetuning Text-to-Image Diffusion Models for Fairness
* Aligning Text-to-Image Models using Human Feedback
* Diffusion Model Alignment Using Direct Preference Optimization
* Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model
* DreamReward- Text-to-3D Generation with Human Preference
* Towards Better Text-to-Image Generation Alignment via Attention Modulation
* PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models
* Rich Human Feedback for Text to Image Generation
* Learning multi-dimensional human preference for text-to-image generation
* A Dense Reward View on Aligning Text-to-Image Diffusion with Preference
  
## Human Alignment Beyond Text-to-Image Diffusion Models 

* DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback
* Flava: A foundational language and vision alignment model
* Text-To-Concept (and Back) via Cross-Model Alignment
* Video Diffusion Alignment via Reward Gradients
* Aligning Target-Aware Molecule Diffusion Models with Exact Energy Optimization

## Fundamentals of Human Alignments

* Learning-to-Rank with Partitioned Preference: Fast Estimation for the Plackett-Luce Model
* Direct Preference Optimization: Your Language Model is Secretly a Reward Model
* RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback
* Preference Ranking Optimization for Human Alignment
* Contrastive Preference Learning: Learning from Human Feedback without RL
* Constitutional AI: Harmlessness from AI Feedback
* Scaling laws for reward model overoptimization
* Training language models to follow instructions with human feedback
* Models of human preference for learning reward functions
* A General Theoretical Paradigm to Understand Learning from Human Preferences
* AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback
* Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision
* Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision
* KTO: Model Alignment as Prospect Theoretic Optimization
* ORPO- Monolithic Preference Optimization without Reference Model
* A General Theoretical Paradigm to Understand Learning from Human Preferences
* Stepwise Alignment for Constrained Language Model Policy Optimization
* The Alignment Problem from a Deep Learning Perspective
* Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints
* Self-Play Preference Optimization for Language Model Alignment
* Direct Language Model Alignment from Online AI Feedback
* SimPO: Simple Preference Optimization with a Reference-Free Reward
* What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning, ICLR2024
* Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs
* Generalized Preference Optimization: A Unified Approach to Offline Alignment
* Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community
  
### Security
* Visual Adversarial Examples Jailbreak Aligned Large Language Models, AAAI 2024, Oral
  
## Evaluation

* DALL-Eval: Probing the reasoning skills and social biases of text-to-image generative transformers
* Evaluating object hallucination in large vision-language models
* Mutual Information Divergence: A Unified Metric for Multimodal Generative Models
* LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation
* VPGen & VPEval: Visual Programming for Text-to-Image Generation and Evaluation
* GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment
* Holistic Evaluation of Text-to-Image Models, NeurIPS2023
* Measuring style similarity in diffusion models
* Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation

## Prompt Optimization
* Improving Text-to-Image Consistency via Automatic Prompt Optimization
* Optimizing Prompts for Text-to-Image Generation
* Model-Agnostic Human Preference Inversion in Diffusion Models
* RePrompt: Automatic Prompt Editing to Refine AI-Generative Art Towards Precise Expressions
* Dynamic Prompt Optimizing for Text-to-Image Generation
* 

## ICML 2024
### Oral
* A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity
* Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study

### Spotlight
* Decoding-time Realignment of Language Models
* Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation
* Model Alignment as Prospect Theoretic Optimization

### Poster
* AI Alignment with Changing and Influenceable Reward Functions
* Understanding the Learning Dynamics of Alignment with Human Feedback
* Fundamental Limitations of Alignment in Large Language Models
* Human Alignment of Large Language Models through Online Preference Optimisation
* Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment
* Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning
* Position: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback
* MaxMin-RLHF: Alignment with Diverse Human Preferences
* Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications
* Towards Efficient Exact Optimization of Language Model Alignment
* Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback
* Generalized Preference Optimization: A Unified Approach to Offline Alignment
* Position: A Roadmap to Pluralistic Alignment

