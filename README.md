# Awesome Alignment of Diffusion Models

The collection of awesome papers on alignment of diffusion model.

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/zeke-xie/awesome-alignment-of-diffusion-models)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Made With Love](https://img.shields.io/badge/Made%20With-Love-red.svg)](https://github.com/chetanraj/awesome-github-badges)

## Human Alignment of Diffusion Models

* Human Preference Score: Better Aligning Text-to-Image Models with Human Preference
* ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation
* Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis
* Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation
* DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models
* Finetuning Text-to-Image Diffusion Models for Fairness
* Aligning Text-to-Image Models using Human Feedback
* Diffusion Model Alignment Using Direct Preference Optimization
* Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model
* DreamReward- Text-to-3D Generation with Human Preference
* Towards Better Text-to-Image Generation Alignment via Attention Modulation
* PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models
* Rich Human Feedback for Text to Image Generation
* Learning multi-dimensional human preference for text-to-image generation
  
## Human Alignment Beyond Text-to-Image Diffusion Models 

* DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback
* Flava: A foundational language and vision alignment model
* Text-To-Concept (and Back) via Cross-Model Alignment
* Video Diffusion Alignment via Reward Gradients
* Aligning Target-Aware Molecule Diffusion Models with Exact Energy Optimization

## Fundamentals of Human Alignments

* Learning-to-Rank with Partitioned Preference: Fast Estimation for the Plackett-Luce Model
* Direct Preference Optimization: Your Language Model is Secretly a Reward Model
* RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback
* Preference Ranking Optimization for Human Alignment
* Contrastive Preference Learning: Learning from Human Feedback without RL
* Constitutional AI: Harmlessness from AI Feedback
* Scaling laws for reward model overoptimization
* Training language models to follow instructions with human feedback
* Models of human preference for learning reward functions
* A General Theoretical Paradigm to Understand Learning from Human Preferences
* AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback
* Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision
* Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision
* Human-Centered Loss Functions (HALOs)
* ORPO- Monolithic Preference Optimization without Reference Model
* A General Theoretical Paradigm to Understand Learning from Human Preferences
* Stepwise Alignment for Constrained Language Model Policy Optimization
* Scaling Laws for Reward Model Overoptimization
* The Alignment Problem from a Deep Learning Perspective
* Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints
* Self-Play Preference Optimization for Language Model Alignment
* Visual Instruction Tuning, NeurIPS 2023, Oral
* Insights into Alignment: Evaluating DPO and its Variants Across Multiple Tasks
* Direct Language Model Alignment from Online AI Feedback
* Understanding the performance gap between online and offline alignment algorithms
* SimPO: Simple Preference Optimization with a Reference-Free Reward
* What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning, ICLR2024
* Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs
  
### Security
* Visual Adversarial Examples Jailbreak Aligned Large Language Models, AAAI 2024, Oral
  
## Evaluation

* DALL-Eval: Probing the reasoning skills and social biases of text-to-image generative transformers
* SelfEval: Leveraging the discriminative nature of generative models for evaluation
* Evaluating object hallucination in large vision-language models
* Mutual Information Divergence: A Unified Metric for Multimodal Generative Models
* LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation
* VPGen & VPEval: Visual Programming for Text-to-Image Generation and Evaluation
* Intriguing Properties of Generative Classifiers
* GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment
* Holistic Evaluation of Text-to-Image Models, NeurIPS2023
* Measuring style similarity in diffusion models
* Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation

## Prompt Optimization
* Improving Text-to-Image Consistency via Automatic Prompt Optimization
* Optimizing Prompts for Text-to-Image Generation
* Model-Agnostic Human Preference Inversion in Diffusion Models
* RePrompt: Automatic Prompt Editing to Refine AI-Generative Art Towards Precise Expressions
* Dynamic Prompt Optimizing for Text-to-Image Generation
