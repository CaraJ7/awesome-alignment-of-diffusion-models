# Awesome Alignment of Diffusion Models

The collection of awesome papers on alignment of diffusion model.

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/zeke-xie/awesome-alignment-of-diffusion-models)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Made With Love](https://img.shields.io/badge/Made%20With-Love-red.svg)](https://github.com/chetanraj/awesome-github-badges)

## Human Alignment of Diffusion Models

* Human Preference Score: Better Aligning Text-to-Image Models with Human Preference
* ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation
* Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis
* Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation
* DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models
* Finetuning Text-to-Image Diffusion Models for Fairness
* Aligning Text-to-Image Models using Human Feedback
* Diffusion Model Alignment Using Direct Preference Optimization
* Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model
* DreamReward- Text-to-3D Generation with Human Preference
* Towards Better Text-to-Image Generation Alignment via Attention Modulation
* PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models
* Rich Human Feedback for Text to Image Generation
* Learning multi-dimensional human preference for text-to-image generation
* A Dense Reward View on Aligning Text-to-Image Diffusion with Preference
  
## Human Alignment Beyond Text-to-Image Diffusion Models 

* DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback
* Flava: A foundational language and vision alignment model
* Video Diffusion Alignment via Reward Gradients
* Aligning Target-Aware Molecule Diffusion Models with Exact Energy Optimization

## Fundamentals of Human Alignments

* Learning-to-Rank with Partitioned Preference: Fast Estimation for the Plackett-Luce Model
* Direct Preference Optimization: Your Language Model is Secretly a Reward Model
* Preference Ranking Optimization for Human Alignment
* Contrastive Preference Learning: Learning from Human Feedback without RL
* Constitutional AI: Harmlessness from AI Feedback
* Scaling laws for reward model overoptimization
* Training language models to follow instructions with human feedback
* Models of human preference for learning reward functions
* A General Theoretical Paradigm to Understand Learning from Human Preferences
* AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback
* Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision
* Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision
* KTO: Model Alignment as Prospect Theoretic Optimization
* ORPO- Monolithic Preference Optimization without Reference Model
* A General Theoretical Paradigm to Understand Learning from Human Preferences
* Stepwise Alignment for Constrained Language Model Policy Optimization
* The Alignment Problem from a Deep Learning Perspective
* Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints
* Self-Play Preference Optimization for Language Model Alignment
* Direct Language Model Alignment from Online AI Feedback
* SimPO: Simple Preference Optimization with a Reference-Free Reward
* What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning, ICLR2024
* Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs
* Generalized Preference Optimization: A Unified Approach to Offline Alignment
* Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community
* Nash Learning from Human Feedback
* Human Alignment of Large Language Models through Online Preference Optimisation
* Active Preference Learning for Large Language Models
* Token-level Direct Preference Optimization
* Dense Reward for Free in Reinforcement Learning from Human Feedback
* RLVF: Learning from Verbal Feedback without Overgeneralization
* Coactive Learning for Large Language Models using Implicit User Feedback
* BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback
* BOND: Aligning LLMs with Best-of-N Distillation

## Understanding and Comparative Analysis

* A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity
* Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study
* Understanding the Learning Dynamics of Alignment with Human Feedback
* Fundamental Limitations of Alignment in Large Language Models
* Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning
* Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences
* Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint
* Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data



## Related Problem of Human Alignment
### Efficiency of Alignment
* Decoding-time Realignment of Language Models
* Towards Efficient Exact Optimization of Language Model Alignment
* Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback
* Feedback Efficient Online Fine-Tuning of Diffusion Models

### RLAIF
* Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation
* RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback
* ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback
* RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback

### Diverse and Changing Preferences
* AI Alignment with Changing and Influenceable Reward Functions
* Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment
* Position: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback
* MaxMin-RLHF: Alignment with Diverse Human Preferences
* Position: A Roadmap to Pluralistic Alignment
* Provably Robust DPO: Aligning Language Models with Noisy Feedback

  
### Security

* Visual Adversarial Examples Jailbreak Aligned Large Language Models, AAAI 2024, Oral
* Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications
  
## Evaluation

* DALL-Eval: Probing the reasoning skills and social biases of text-to-image generative transformers
* Evaluating object hallucination in large vision-language models
* Mutual Information Divergence: A Unified Metric for Multimodal Generative Models
* LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation
* VPGen & VPEval: Visual Programming for Text-to-Image Generation and Evaluation
* GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment
* Holistic Evaluation of Text-to-Image Models, NeurIPS2023
* Measuring style similarity in diffusion models
* Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation
* Quality Diversity through Human Feedback: Towards Open-Ended Diversity-Driven Optimization

## Prompt Optimization

* Improving Text-to-Image Consistency via Automatic Prompt Optimization
* Optimizing Prompts for Text-to-Image Generation
* Model-Agnostic Human Preference Inversion in Diffusion Models
* RePrompt: Automatic Prompt Editing to Refine AI-Generative Art Towards Precise Expressions
* Dynamic Prompt Optimizing for Text-to-Image Generation
