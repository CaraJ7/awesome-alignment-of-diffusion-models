# Awesome Alignment of Diffusion Models

The collection of awesome papers on alignment of diffusion model.

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/zeke-xie/awesome-alignment-of-diffusion-models)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Made With Love](https://img.shields.io/badge/Made%20With-Love-red.svg)](https://github.com/chetanraj/awesome-github-badges)

## Human Alignment of Diffusion Models

* Human Preference Score: Better Aligning Text-to-Image Models with Human Preference
* ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation
* Human Preference Score v2: A Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis
* Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation
* DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models
* Finetuning Text-to-Image Diffusion Models for Fairness
* Aligning Text-to-Image Models using Human Feedback
* Diffusion Model Alignment Using Direct Preference Optimization
* Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model
* DreamReward- Text-to-3D Generation with Human Preference

## Human Alignment of General Vision-Language Models

* DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback
* Flava: A foundational language and vision alignment model
* Text-To-Concept (and Back) via Cross-Model Alignment

## Fundamentals of Human Alignments

* Learning-to-Rank with Partitioned Preference: Fast Estimation for the Plackett-Luce Model
* Direct Preference Optimization: Your Language Model is Secretly a Reward Model
* RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback
* Preference Ranking Optimization for Human Alignment
* Contrastive Preference Learning: Learning from Human Feedback without RL
* Constitutional AI: Harmlessness from AI Feedback
* Scaling laws for reward model overoptimization
* Training language models to follow instructions with human feedback
* Models of human preference for learning reward functions
* A General Theoretical Paradigm to Understand Learning from Human Preferences
* AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback
* Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision
* Human-Centered Loss Functions (HALOs)
* ORPO- Monolithic Preference Optimization without Reference Model
* A General Theoretical Paradigm to Understand Learning from Human Preferences

## Evaluation

* DALL-Eval: Probing the reasoning skills and social biases of text-to-image generative transformers
* SelfEval: Leveraging the discriminative nature of generative models for evaluation
* Evaluating object hallucination in large vision-language models
* Mutual Information Divergence: A Unified Metric for Multimodal Generative Models
* LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation
* VPGen & VPEval: Visual Programming for Text-to-Image Generation and Evaluation
* Intriguing Properties of Generative Classifiers
